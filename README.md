# Determinet

üì¶ Be sure to check out the NuGet package: https://www.nuget.org/packages/NTDLS.Determinet

Determinet is versatile multilayer perception neural network designed for ease of use and extendibility.
In addition the the library, you'll find a test harness which includes a character recognition trainer, validator and visual testing tool.
These provide working examples of training a model as well as generating predictions.

## Built-in Activation Functions
| Activation Type                   | What It Does                                                                                      | When To Use It                                                                                                                                                          | Where To Use It                                                             |
| --------------------------------- | ------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |
| **Identity**                      | Passes values through unchanged. The neuron is basically ‚Äútransparent.‚Äù                           | Use for **pure regression** or as a **linear output layer** when predicting real numbers ‚Äî e.g., predicting house prices, speed, or temperature.                        | Output                                                                      |
| **PiecewiseLinear**               | Acts linear within a range but limits slope outside (like a gentle LeakyReLU with bounds).        | Use when you want **bounded but mostly linear** responses ‚Äî e.g., controlling actuator strength, image intensity normalization, or testing stable transitions.          | Hidden (sometimes Output for bounded regression)                            |
| **Linear**                        | Scales input by a slope and optionally clamps to a range.                                         | Great for **controlled continuous outputs** where scaling matters ‚Äî e.g., steering angle, torque output, or color channel prediction.                                   | Output (regression), Hidden (feature scaling)                               |
| **ReLU (Rectified Linear Unit)**  | Keeps positive values, zeros out negatives. Very fast and stable.                                 | The **default** for most deep learning tasks. Ideal for **vision models, embeddings, and dense MLPs** ‚Äî e.g., image recognition, feature extraction, signal processing. | Hidden                                                                      |
| **Sigmoid**                       | Squashes output between 0 and 1 (probability curve).                                              | Use for **binary classification** (‚Äúyes/no,‚Äù ‚Äúon/off,‚Äù ‚Äúspam/not spam‚Äù) or when modeling **probability per neuron** in multi-label tasks.                               | Output (binary classification), Hidden (gating in LSTMs or attention)       |
| **Tanh**                          | Squashes output between -1 and 1, centered around 0.                                              | Use when values can be **positive or negative** and symmetry matters ‚Äî e.g., **RNNs**, control systems, or **directional outputs** (-1 left, +1 right).                 | Hidden                                                                      |
| **LeakyReLU**                     | Like ReLU, but negative values leak through slightly instead of being zeroed.                     | Use in **deep MLPs** to avoid ‚Äúdead neurons‚Äù and keep gradients flowing ‚Äî especially in **dense** or **deep** architectures.                                            | Hidden                                                                      |
| **SoftMax**                       | Converts outputs into a probability distribution that sums to 1.                                  | Use for **multi-class classification** ‚Äî e.g., recognizing characters, digits, objects, or anything where exactly one class is correct.                                 | Output                                                                      |
| **SimpleSoftMax**                 | A simplified SoftMax (no temperature scaling).                                                    | Use when you need **fast, stable classification** and temperature tuning isn‚Äôt required. Works the same for most classification tasks.                                  | Output                                                                      |
| **ELU (Exponential Linear Unit)** | Works like ReLU for positive inputs but smoothly bends for negatives instead of dropping to zero. | Use when you want **ReLU-like behavior** but prefer smoother gradients and **mean activations closer to zero** ‚Äî e.g., deep networks prone to ‚Äúdead neurons.‚Äù           | Hidden                                                                      |
| **Gaussian (RBF)**                | Outputs a bell curve centered around zero (f(x) = e<sup>-x¬≤</sup>).                               | Use for **radial basis networks**, **feature clustering**, or **anomaly detection** where proximity to a center value matters.                                          | Hidden                                                                      |
| **HardSigmoid**                   | A linear, fast approximation of the sigmoid function.                                             | Use in **lightweight models** or embedded systems where performance matters, or when full sigmoid precision isn‚Äôt required.                                             | Hidden, Output (binary)                                                     |
| **HardTanh**                      | Like Tanh but clipped to the range [-1, 1] using straight lines.                                  | Use when you need **bounded outputs** but want faster computation ‚Äî e.g., control tasks, quantized models, or when smoothness isn‚Äôt critical.                           | Hidden                                                                      |
| **Mish**                          | Smoothly blends ReLU and Swish behavior (f(x) = x¬∑tanh(ln(1+eÀ£))).                                | Use when you want **smoother gradients** and slightly better **generalization** in deep MLPs or CNNs ‚Äî can outperform ReLU/Swish on some tasks.                         | Hidden                                                                      |
| **SELU (Scaled ELU)**             | A self-normalizing version of ELU that keeps activations around mean=0, var=1.                    | Use in **deep fully-connected networks** without batch normalization; helps stabilize training automatically.                                                           | Hidden                                                                      |
| **SoftSign**                      | Smoothly scales inputs as x / (1 +                                                                | Use as a **lightweight, stable alternative** to tanh when you want smooth gradients without exponential cost ‚Äî good for RNNs or bounded outputs.                        | Hidden                                                                      |
| **Swish**                         | Smooth, self-gated function (f(x) = x ¬∑ sigmoid(x)).                                              | Use in **modern deep networks** (CNNs, Transformers, MLPs) where you want ReLU-like performance but with **better gradient flow** and smoother transitions.             | Hidden                                                                      |
| **SoftPlus**                      | Smooth version of ReLU (f(x) = ln(1 + eÀ£)); never zeroes out completely.                          | Use when you want **ReLU-like behavior** but need continuous derivatives ‚Äî e.g., **probabilistic networks**, VAEs, or any model requiring gradient stability.           | Hidden, sometimes Output (positive-only regression)                         |
